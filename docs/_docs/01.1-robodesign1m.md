---
title: "RoboDesign1M"
permalink: /docs/robodesign1m/
excerpt: "Overview of RoboDesign1M."
redirect_from:
  - /theme-setup/
toc: true
---


## Dataset Comparison
<p align="center">
  <img src="../../assets/images/intro/dataset-comparison.png" alt="dataset_comparison" style="width: 100%;" />
</p>
RoboDesign1M dataset is collected from over 1M scientific works. With **1 million** samples and multimodal ground truth, it is the first large-scale dataset dedicated for robot design understanding.
<!-- [[1]](#references) -->

## Statistics

<p align="center">
  <img src="../../assets/images/intro/unique-word.png" alt="num-objects" style="width: 49%;" />
  <img src="../../assets/images/intro/keyword-histogram.png" alt="num-samples" style="width: 50%;" />
</p>

RoboDesign1M significantly covers a wide range of keywords and outperforms other datasets in terms of vocabulary size.

## Data Pipeline
<p align="center">
  <img src="../../assets/images/robodesign1m/data-collection-pipeline.png" alt="data_pipeline" style="width: 100%;" />
</p>


We design a semi-automated data collection pipeline. Specifically, we collect and extract images, texts data from scientific documents. In order to filter images related to robot design, we manually annotate 32K images and train a classification model. Next, we clean the extracted images with the trained model, resulting in robot design image-text pairs data. In addition, we construct visual-instruction tuning data by using a high-performing open-sourced large language model to generate over 1M question-answer pairs.


## Samples
<p align="center">
  <img src="../../assets/images/robodesign1m/example-conversation.png" alt="grasp-anything-sample" style="width: 100%;" />
</p>

## Demonstration
<video width="100%" controls>
  <source src="../../assets/images/robodesign1m/demo-robodesign1.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>


<!-- ## References
<a name="references"></a> -->

<!-- - [1] [An Dinh Vuong](https://andvg3.github.io/), Minh Nhat Vu, Hieu Le, Baoru Huang, Binh Huynh, Thieu Vo, Andreas Kugi, [Anh Nguyen](https://www.csc.liv.ac.uk/~anguyen/). *Grasp-anything: Large-scale grasp dataset from foundation models*. In ICRA, 2024. -->
<!-- 
- [2] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Omme. *High-resolution image synthesis with latent diffusion models*. In CVPR, 2020. -->